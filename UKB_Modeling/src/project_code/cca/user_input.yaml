### Define your project and the preprocessing run you want to use:
export_date_data: "Non_screened" #Reflects to the folder name where your data is stored in (after preprocessing)
DOI: "CCA" # DOI of the project, used for folder structure
project: "cca" # Name of the project, used for folder structure

# Define the cohort you want to use:
row_subset: "unscreened_cca"  # "par" or "all" or "par_Cirrhosis"
col_subset: "Model_A" # Change this line to select another model combination
test_cohort: "val" # applying model to either "test" or "val" data
visual_export: true # Decide whether you just want to play around or export your visuals
run_conf_matrices: true
run_violin_plot: true
run_feature_imp: true
RUN_MODELS: true  # True or False, set on False if just evaluating
add_benchmark: false
target: "status" # status, status_cancerreg or whatever you want to call the relevant column
target_to_validate_on: "status_cancerreg"
method_scaling_remainders: "no_scaling" # "no_scaling", "standard" or "minmax"
time_of_readout: # List of floating point numbers of years, or None to use all data -> only relevant if you call the time_dep_training_tube -> train on differnt time points:)
- 5
- 10
use_raw: False # True or False, if you want to use the raw data or previously normalized data
use_adjusted_ohe: False # True or False, if you want to use the adjusted package for the ohe, with the adjusted one you can exclude specific transformed columns from the analysis.


# Set up some color mappings for the visuals
color_groups_all:
  df_covariates: "#A6CEE3"
  df_diagnosis: "#FFC766"
  df_blood: "#FB9A99"
  df_snp: "#B2DF8A"
  df_metabolomics: "#C79FD6"
  df_metadata: "#FBB799"
  Model_A: "#A6CEE3"
  Model AVec: "#FFC766"
  Model_B: "#FFC766"
  Model_C: "#FB9A99"
  Model_D: "#B2DF8A"
  Model_E: "#C79FD6"
  TOP75: "#FBB799"
  TOP30: "#FBD499"
  TOP20: "#FAEC7E"
  TOP10: "#ff6666"
  TOP5: "#6666ff"
  TOP10 - PMBB: "#6666ff"
  TOP10 - AOU: "#30cc62"
  TOP5 - PMBB : "#6666ff"
  TOP5 - AOU: "#30cc62"
  Roc: "#1f77b4"
  Prc: "#ff7f0e"




color_groups_violin:
  Model_A: [0.28627450980392155, 0.5843137254901961, 0.6784313725490196]
  Model_B: [0.2196078431372549, 0.3333333333333333, 0.4745098039215686]
  Model_C: [0.7568627450980392, 0.21176470588235294, 0.09019607843137255]
  Model_D: [0.9411764705882353, 0.5647058823529412, 0.24313725490196078]
  Model_E: [0.9411764705882353, 0.7843137254901961, 0.4470588235294118]
  Model_TOP75: [0.796078431372549, 0.4, 0.29411764705882354]
  Model_TOP30: [0.807843137254902, 0.5647058823529412, 0.4980392156862745]


# The hyperparameter configuration is explicitly organized into three clearly separated sections for each model:
#1. params_fixed  (Never change during hyperparameter search): these get passed to the model call itself
# (via get_estimator function in pipeline.py, will be appended as arguments e.g. for "RFC": RandomForestClassifier
# -> return models[label](**fixed_params)) (returns all fixed parameters)

# 2. params_grid  (Are explicitly searched/tuned via grid search, if just one item, GridSearch is redundant)
#3. params_pipeline (Define pipeline-level evaluation and validation hyperparameters)
# These sections distinguish between parameters that:


hyperparameters:
  RFC:
    params_fixed:
      random_state: 42                      # Random state for reproducibility
      class_weight: "balanced_subsample"               # Class weight for imbalanced data
    params_grid:
      max_depth: [3]                   # Maximum depth of the tree
      n_estimators: [100]            # Number of trees in the forest
    params_pipeline:
      cross_validation_method: "grouped"    # Cross-validation method, grouped or split, grouped is recommended as of TRIPOD
      n_splits: 5                           # Number of splits for cross-validation (=folds)
      n_jobs_indiv: -1                       # Number of jobs to run in parallel, -1 means all processors
      scoring_grid_search: "balanced_accuracy" # Scoring metric for grid search
      verbose: 2                     # Verbosity level for grid search

  XGB:
    params_fixed:
      random_state: 42                      # Random state for reproducibility
      learning_rate: 0.1
    params_grid:
      max_depth: [3, 5, 8]                   # Maximum depth of the tree
      n_estimators: [100, 200, 500]            # Number of trees in the forest
      learning_rate: [0.01, 0.05, 0.1]
    params_pipeline:
      cross_validation_method: "grouped"    # Cross-validation method, grouped or split, grouped is recommended as of TRIPOD
      n_splits: 5                           # Number of splits for cross-validation (=folds)
      n_jobs_indiv: -1                       # Number of jobs to run in parallel, -1 means all processors
      scoring_grid_search: "balanced_accuracy" # Scoring metric for grid search
      verbose: 2

  CatBoost:
    params_fixed:
      random_state: 42
      class_weights: [1.0, 750] # or 51299 / 211 Class weights for imbalanced data
      verbose: 2
    params_grid:
      iterations: [200]
      depth: [5]
      learning_rate: [0.01]
    params_pipeline:
      scoring_grid_search: "balanced_accuracy"
      cross_validation_method: "grouped"
      n_splits: 5
      n_jobs_indiv: -1
      verbose: 2

  ExtraTrees:
    params_fixed:
      random_state: 42
      class_weight: "balanced_subsample"
    params_grid:
      n_estimators: [100, 200, 500]
      criterion: ["gini", "entropy"]
      max_features: ["sqrt", "log2"]
      max_depth: [None, 3, 5]
    params_pipeline:
      cross_validation_method: "grouped"
      n_splits: 5
      n_jobs_indiv: -1
      scoring_grid_search: "balanced_accuracy"
      verbose: 2

  neuronMLP: # Multi-layer Perceptron
    params_fixed:
      random_state: 42
    params_grid:
      hidden_layer_sizes: [(100, 50, 20, 5), (100, 50, 10), (100, 50), (100, 20), (100, 10), (100)]
      activation: ["relu"] #, "tanh", "logistic"]
      solver: ["adam"] #, "sgd"]
    params_pipeline:
      cross_validation_method: "grouped"




